name: Run vLLM Benchmark

on:
  workflow_dispatch:
    inputs:
      gpu_type:
        description: "GPU Type"
        required: false
        default: "NVIDIA RTX 2000 Ada Generation"
      model:
        description: "Model to benchmark"
        required: true
        default: "neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8"
      max_model_len:
        description: "Max model context length (input + output tokens)"
        required: false
        default: "32768"  
      dataset_name:
        description: "Dataset Name"
        required: false
        default: "random"
      dataset_path:
        description: "Dataset Path"
        required: false
        default: ""        
      num_prompts:
        description: "Number of prompts"
        required: false
        default: "20"
      input_len:
        description: "Input length (tokens)"
        required: false
        default: "1024"
      output_len:
        description: "Output length (tokens)"
        required: false
        default: "256"
      extra_args:
        description: "Additional vLLM CLI arguments (optional)"
        required: false
        default: ""        

jobs:
  benchmark:
    name: Run vLLM Benchmark
    runs-on: ubuntu-latest
    env:
      IMAGE_NAME: "vllm/vllm-openai:latest"
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Install RunPod CLI
        run: wget --quiet --show-progress https://github.com/Run-Pod/runpodctl/releases/download/v1.14.3/runpodctl-linux-amd64 -O runpodctl && chmod +x runpodctl && sudo cp runpodctl /usr/bin/runpodctl

      - name: Configure RunPod CLI
        run: runpodctl config --apiKey ${{ secrets.API_KEY }}
        continue-on-error: true

      - name: Create RunPod Instance
        run: |
          CREATE_OUTPUT=$(runpodctl create pod \
            --ports "8000/tcp" \
            --secureCloud \
            --gpuType "${{ inputs.gpu_type }}" \
            --gpuCount 2 \
            --imageName "$IMAGE_NAME" \
            --args "--model ${{ inputs.model }} --max-model-len ${{ inputs.max_model_len }} --port 8000 --gpu-memory-utilization 0.95")
          POD_ID=$(echo "$CREATE_OUTPUT" | awk -F'"' '/pod "/ {print $2}')
          echo "POD_ID=$POD_ID" >> $GITHUB_ENV

      - name: Wait for RunPod Instance
        run: sleep 220

      - name: Retrieve RunPod Endpoint
        run: |
          ENDPOINT=$(echo "$(runpodctl get pod $POD_ID -a)" | grep -oP '\d+\.\d+\.\d+\.\d+:\d+(?=->8000)')
          echo "ENDPOINT=$ENDPOINT" >> $GITHUB_ENV

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"

      - name: Prepare dataset args
        run: |
          DATASET_ARGS="--dataset-name ${{ inputs.dataset_name }}"
          if [ -n "${{ inputs.dataset_path }}" ]; then
            DATASET_ARGS="$DATASET_ARGS --dataset-path ${{ inputs.dataset_path }}"
          fi
          echo "DATASET_ARGS=$DATASET_ARGS" >> $GITHUB_ENV
    
      - name: Install uv
        run: |
          python -m pip install uv
          
      - name: Install and Run vLLM Benchmark
        run: |
          uv venv vllm-env --python 3.12 --seed
          source vllm-env/bin/activate
          uv pip install datasets
          uv pip install vllm
          vllm bench serve \
            --backend openai \
            --base-url "http://$ENDPOINT" \
            $DATASET_ARGS \
            --model ${{ inputs.model }} \
            --num-prompts ${{ inputs.num_prompts }} \
            --seed 12345 \
            --input-len ${{ inputs.input_len }} \
            --output-len ${{ inputs.output_len }} \
            ${{ inputs.extra_args }} \
            --save-result \
            --result-filename benchmark_result.json \
            | tee benchmark_output.log

      - name: Generate Benchmark Summary
        run: |
          GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader | head -n 1 2>/dev/null || echo "n/a")
          {
            echo "## vLLM Benchmark Result"
            echo ""
            echo "### Run config"
            echo "- **GPU (input):** ${{ inputs.gpu_type }}"
            echo "- **GPU (nvidia-smi):** ${GPU_NAME}"
            echo "- **Model:** ${{ inputs.model }}"
            echo "- **Server max_model_len:** ${{ inputs.max_model_len }}"
            echo "- **Bench input_len:** ${{ inputs.input_len }}"
            echo "- **Bench output_len:** ${{ inputs.output_len }}"
            echo ""
            echo "### Metrics"
            echo '```'
            grep -A 200 "============ Serving Benchmark Result" benchmark_output.log || true
            echo '```'
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark_result.json

      - name: Remove RunPod Pod
        if: always()
        run: runpodctl remove pod $POD_ID
