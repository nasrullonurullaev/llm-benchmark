name: Run LM Evaluation Harness Benchmark

on:
  workflow_dispatch:
    inputs:
      model:
        description: "Model to benchmark"
        required: true
        default: "neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8"
      dataset_task:
        description: "Dataset task to evaluate"
        required: true
        default: "humaneval"

jobs:
  benchmark:
    name: Run LM Evaluation Harness Benchmark
    runs-on: ubuntu-latest
    env:
      GPU_TYPE: "NVIDIA RTX 2000 Ada Generation"
      IMAGE_NAME: "vllm/vllm-openai:latest"
      HF_ALLOW_CODE_EVAL: "1"
      OPENAI_API_KEY: "${{ secrets.LLM_API_KEY }}"
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Install RunPod CLI
        run: wget -qO- cli.runpod.net | sudo bash

      - name: Configure RunPod CLI
        run: runpodctl config --apiKey ${{ secrets.API_KEY }}
        continue-on-error: true

      - name: Create RunPod Instance
        run: |
          CREATE_OUTPUT=$(runpodctl create pod \
            --ports "8000/tcp" \
            --secureCloud \
            --gpuType "$GPU_TYPE" \
            --imageName "$IMAGE_NAME" \
            --args "--model ${{ inputs.model }} --max-model-len 4096 --port 8000 --enforce-eager --gpu-memory-utilization 0.95 --api-key $OPENAI_API_KEY")
          POD_ID=$(echo "$CREATE_OUTPUT" | awk -F'"' '/pod "/ {print $2}')
          echo "POD_ID=$POD_ID" >> $GITHUB_ENV

      - name: Wait for RunPod Instance
        run: sleep 150

      - name: Retrieve RunPod Endpoint
        run: |
          ENDPOINT=$(runpodctl get pod $POD_ID -a | grep -oP '\d+\.\d+\.\d+\.\d+:\d+(?=->8000)')
          echo "ENDPOINT=$ENDPOINT" >> $GITHUB_ENV

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"

      - name: Clone and Install LM Evaluation Harness
        run: |
          git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness
          cd lm-evaluation-harness
          pip install -e .
          pip install -e ."[api]"

      - name: Run LM Evaluation Harness Benchmark
        run: |
          lm_eval \
            --model local-completions \
            --tasks ${{ inputs.dataset_task }} \
            --trust_remote_code \
            --confirm_run_unsafe_code \
            --log_samples \
            --output_path /tmp/lm-evaluation-harness/ \
            --limit 2 \
            --model_args model=${{ inputs.model }},base_url=http://$ENDPOINT/v1/completions,num_concurrent=1,max_retries=3,tokenized_requests=False | tee benchmark_output.log
      
      - name: Output Benchmark Results to Summary
        run: |
          echo "### Benchmark Results" >> $GITHUB_STEP_SUMMARY
          tail -n 10 benchmark_output.log | awk '/\| *Tasks *\|Version\|/{flag=1} flag' >> $GITHUB_STEP_SUMMARY

      - name: Remove RunPod Pod
        if: always()
        run: runpodctl remove pod $POD_ID

      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: /tmp/lm-evaluation-harness
